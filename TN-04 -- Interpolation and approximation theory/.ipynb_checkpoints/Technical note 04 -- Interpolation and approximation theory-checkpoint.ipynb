{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximation theory\n",
    "\n",
    "we quote the following for motivation: \"For every predefined table of interpolation nodes there is a continuous function for which the sequence of interpolation polynomials on those nodes diverges.[6] For every continuous function there is a table of nodes on which the interpolation process converges.[citation needed] Chebyshev interpolation (i.e., on Chebyshev nodes) converges uniformly for every absolutely continuous function.\"\n",
    "\n",
    "That is, we seek a function $P(x)$ that interpolates a function $f$ of $x$ over a domain $\\mathcal{D}$ using representative sample points (nodes) $x_i$ in the domain which the function $P$ is designed to agree with exactly. That The approximation theory problem is to find such a function that minimizes a measure of the error, e.g.\n",
    "\n",
    "$$\\min_{x\\in\\mathcal{D}}||P(x) - f(x)||$$\n",
    "\n",
    "e.g. if we wish to minimize the maximum error, we use the $L^{\\infty}$ norm to a small number $\\varepsilon$,\n",
    "\n",
    "$$\\min_{x\\in\\mathcal{D}}\\left\\{ \\max_{x\\in\\mathcal{D}} |P(x) - f(x)|\\right\\} = \\varepsilon$$\n",
    "\n",
    "It is a fundamental result of approximation theory that a polynomial, for example, of degree $N$, $P_N$ can interpolate $N+1$ points exactly. In the course of finding such a polynomial, an <b>optimal polynomial</b> of degree $N$, $P_N$, is such a function that can interpolate $N+1$ points and which is said to <b>level</b>, i.e. it's error curve oscillates betwen $\\pm \\varepsilon$, $\\varepsilon\\in\\mathbb{R}$ and has $N+2$ extrema. It this latter point that gives rise to <b>Runge's phenomenon</b>, a phenomenon whereby increasing the number of points (hence, the order of the polynomial $N$) on a uniformly spaced grid decreases the agreement with the actual function due to oscillating between the $N+2$ extrema. These tend to \"accumulate\" near the node edges, so that a ringing phenomenon is observed at the edges of the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrange polynomials\n",
    "\n",
    "We consider the set of data: $(x_0, y_0),\\ldots ,(x_j, y_j),\\ldots ,(x_k, y_k)$\n",
    "\n",
    "The minimum degree polynomial hat interpolates each datum $(x_j, y_j)$ exactly can be expressed in so-called <i>Lagrange form</i> as:\n",
    "\n",
    "$$L(x) := \\sum_{j=0}^{k} y_j \\ell_j(x)$$\n",
    "\n",
    "where the basis functions are termed <i>Lagrange polynomials</i>\n",
    "\n",
    "$$\\ell_j(x) := \\prod_{\\begin{smallmatrix}0\\le m\\le k\\\\ m\\neq j\\end{smallmatrix}} \\frac{x-x_m}{x_j-x_m} = \\frac{(x-x_0)}{(x_j-x_0)} \\cdots \\frac{(x-x_{j-1})}{(x_j-x_{j-1})} \\frac{(x-x_{j+1})}{(x_j-x_{j+1})} \\cdots \\frac{(x-x_k)}{(x_j-x_k)}$$\n",
    "\n",
    "It is obvious that\n",
    "\n",
    "$$\\ell_j(x_i) = \\delta_{ij}$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$L(x_i) = \\sum_{j = 0}^k y_j \\ell_j(x_i) = \\sum_{j = 0}^k y_j \\delta_{ij} = y_i$$\n",
    "\n",
    "So it is seen that $L$ is an interpolant for the data $(x_i,y_i)$, $i = 0, 1, \\ldots , k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrange basis vs. monomial basis\n",
    "\n",
    "Naive procedures to obtain an interpolant of minimum degree involves considering solving for the $k+1$ coefficientss $\\{c_j\\}$ in the assembled polynomial:\n",
    "\n",
    "$$\\text{find } P(x) = \\sum_{j = 0}^k c_j x^j \\qquad \\text{such that } P(x_j) = y_j$$\n",
    "\n",
    "It is clear this requires $k+1$ equations for a unique solution, hence we require $k+1$ data points $(x_j, y_j)$ for $j = 0, 1, \\ldots k$. Writing a few of the equations:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P(x_0) = c_0 + c_1x_0 + c_2x_0^2 + \\ldots c_k x_0^k & = & y_0 \\\\\n",
    "&& \\\\\n",
    "P(x_1) = c_0 + c_1x_1 + c_2x_1^2 + \\ldots c_k x_1^k & = & y_1 \\\\\n",
    "&&\\\\\n",
    "P(x_2) = c_0 + c_1x_2 + c_2x_2^2 + \\ldots c_k x_2^k & = & y_2 \\\\\n",
    "&& \\\\\n",
    "\\phantom{P(x_2) = c_0 + c_1x_2} \\vdots \\phantom{c_2x_2^2 + \\ldots c_k x_2^k } & = & \\phantom{y}\\vdots \\\\\n",
    "&& \\\\\n",
    "P(x_k) = c_0 + c_1x_k + c_2x_k^2 + \\ldots c_k x_k^k & = & y_k \\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "when casted as a matrix problem, we arrive at a <i>Vandermonde</i> matrix, $\\underline{\\underline{V}}$:\n",
    "\n",
    "$$\\underline{\\underline{V}}\\cdot \\underline{c} = \\underline{y}$$\n",
    "\n",
    "where $\\underline{c} = (c_0, c_1, \\ldots , c_k)^T$, $\\underline{y} = (y_0, y_1, \\ldots , y_k)^T$, and\n",
    "\n",
    "$$\\underline{\\underline{V}}_{(k+1)\\times (k+1)} = \\left( \\begin{array}{ccccc}\n",
    "1 & x_0 & x_0^2 & \\cdots & x_0^k \\\\\n",
    "1 & x_1 & x_1^2 & \\cdots & x_1^k \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\cdots & \\cdots \\\\\n",
    "1 & x_k & x_k^2 & \\cdots & x_k^k \\end{array} \\right)$$\n",
    "\n",
    "In general, this matrix is dense and the condition number $\\kappa$ can be quite large so the errors from numerical methods, e.g. inversion of $\\underline{\\underline{V}}$, are amplified significantly. Also, matrix inversion is at worst $O(k^3)$ operations.\n",
    "\n",
    "The advantage of the judicious choice of Lagrange basis functions invites the similar problem:\n",
    "\n",
    "$$\\text{find } L(x) = \\sum_{j = 0}^k c_j\\ell_j (x) \\qquad \\text{such that } L(x_j) = y_j$$\n",
    "\n",
    "Here, we have used perhaps the most powerful technique in mathematics, that is we \"write down the answer.\" We prove this is the proper choice a posteriori now by writing out several equations and showing all coefficients $c_j = 1$ for $j = i$. Recalling that $L(x_i) = \\sum_{j = 0}^k \\delta_{ij} y_j$,\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "L(x_0) = c_0\\delta_{00} + 0 + 0 + \\ldots 0 & = & y_0 \\\\\n",
    "&& \\\\\n",
    "L(x_1) = 0 + c_1\\delta_{11} + 0 + \\ldots 0 & = & y_1 \\\\\n",
    "&&\\\\\n",
    "L(x_2) = 0 + 0 + c_2\\delta_{22} + 0 & = & y_2 \\\\\n",
    "&& \\\\\n",
    "\\phantom{L(x_2) = 0 + 0} \\vdots \\phantom{c_2\\delta_{22} + 0} & = & \\phantom{y}\\vdots \\\\\n",
    "&& \\\\\n",
    "L(x_k) = 0 + 0 + 0 + \\ldots c_k\\delta_{kk} & = & y_k \\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "or in matrix form\n",
    "\n",
    "$$\\underline{\\underline{I}}\\cdot \\underline{c} = \\underline{y}$$\n",
    "\n",
    "where $\\underline{\\underline{I}}{^{-1}} = \\underline{\\underline{I}}$ is the identity matrix. It follows then that $\\underline{c} = \\underline{y}$ or $c_j = y_j$, $j = 0, 1, \\ldots , k$ and so the interpolant takes the form $L(x) = \\sum_{j = 0}^k \\ell_j(x) y_j$ as posited. Thus, we evade a steep computational cost by an astute choice of basis polynomial that interpolates each datum by construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolating on uniform grids: Runge's phenomenon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Runge's phenomenon. (2015, May 21). In Wikipedia, The Free Encyclopedia. Retrieved 22:32, July 6, 2015, from https://en.wikipedia.org/w/index.php?title=Runge%27s_phenomenon&oldid=663444092\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
